{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import random\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener el contenido HTML de la pagina web \n",
    "res = urllib.request.urlopen(\"https://www.ionos.es/digitalguide/paginas-web/desarrollo-web/que-es-el-web-scraping/\")\n",
    "html = res.read()\n",
    "soup = BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'source', 'style', 'article', 'nav', 'body', 'button', 'i', 'a', 'br', 'html', 'link', 'form', 'input', 'h1', 'h3', 'time', 'label', 'header', 'ol', 'img', 'noscript', 'h2', 'ul', 'picture', 'h4', 'main', 'li', 'head', 'div', 'base', 'p', 'meta', 'section', 'footer', 'span', 'title', 'fieldset', 'em', 'script', 'strong'}\n"
     ]
    }
   ],
   "source": [
    "#observamos la etiquetas HTML\n",
    "setTag = {x.name for x in soup.find_all(True)}\n",
    "print(setTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Los motores de búsqueda, como Google, utilizan desde hace tiempo los denominados rastreadores web o crawlers, que exploran Internet en busca de términos definidos por el usuario. Los rastreadores son tipos especiales de bots, que visitan una página web tras otra para generar asociaciones con términos de búsqueda y categorizarlos. El primer rastreador web se creó ya en 1993, cuando se presentó el primer motor de búsqueda: Jumpstation.\n\nEntre estas técnicas de rastreo se incluye el web scraping o webharvesting. Te explicamos cómo funciona, para qué se utiliza y cómo se puede bloquear en caso necesario.\n\nDurante el web scraping (del inglés scraping = arañar/raspar) se extraen y almacenan datos de páginas web para analizarlos o utilizarlos en otra parte. Por medio de este raspado web se almacenan diversos tipos de información: por ejemplo, datos de contacto, tales como direcciones de correo electrónico o números de teléfono, o también términos de búsqueda o URL. Estos se almacenan en bases de datos locales o tablas.\n\nCon el web scraping se leen textos de páginas web para obtener información y almacenarla, de forma comparable al proceso automático de copiado y pegado. En el caso de la búsqueda de imágenes, el proceso se denomina acertadamente image scraping.\n\nDentro del scraping hay diferentes modos de funcionamiento, aunque en general se diferencia entre el scraping automático y el manual. El scraping manual define el copiado y pegado manual de información y datos, como quien recorta y guarda artículos de periódico y solo se lleva a cabo si se desea encontrar y almacenar alguna información concreta. Es un proceso muy laborioso que raras veces se aplica a grandes cantidades de datos.\n\nEn el caso del scraping automático, se recurre a un software o un algoritmo que analiza diferentes páginas web para extraer información. Se utiliza software especializado según el tipo de página web y el contenido. Dentro del scraping automático, se diferencian varios modos de proceder:\n\nEn este tutorial te enseñamos qué hay que tener en cuenta para el web scraping con Python. Para recopilar los datos, puede integrarse el WebDriver de Selenium sin ningún problema.\n\nEl web scraping se utiliza para una gran variedad de tareas, por ejemplo, para recopilar datos de contacto o información especial con gran rapidez. En el ámbito profesional, el scraping se utiliza a menudo para obtener ventajas respecto a la competencia. De esta forma, por medio del harvesting de datos, una empresa puede examinar todos los productos de un competidor y compararlos con los propios. El web scraping también resulta valioso en relación con los datos financieros: es posible leer datos desde un sitio web externo, organizarlos en forma de tabla y después analizarlos y procesarlos.\n\nGoogle es un buen ejemplo de web scraping. El buscador utiliza esta tecnología para mostrar información meteorológica o comparaciones de precios de hoteles y vuelos. Muchos de los portales actuales de comparación de precios también utilizan el scraping para representar información de diferentes sitios web y proveedores.\n\nEl scrapingno siempre es legal. En primer lugar, los scrapers deben tener en cuenta los derechos de propiedad intelectual de los sitios web. El web scraping tiene consecuencias muy negativas para algunas tiendas online y proveedores, por ejemplo, si el posicionamiento de su página se ve afectado debido a los agregadores. No es raro, por lo tanto, que una empresa se querelle contra un portal de comparaciones para impedir el web scraping. En uno de estos casos, el Tribunal Superior Regional de Fráncfort dictaminó en 2009 que una compañía aérea debía permitir el scraping por parte de portales comparativos porque, al fin y al cabo, su información es de libre acceso. No obstante, la aerolínea tenía la posibilidad de recurrir a medidas técnicas para evitarlo.\n\nEl scraping es legal, por lo tanto, siempre y cuando los datos recabados estén disponibles libremente para terceros en la web. Para garantizar la legalidad del web scraping, hay que tener en consideración lo siguiente:\n\nAunque el scraping está permitido en muchos casos, puede utilizarse con fines destructivos o incluso ilegales. Por ejemplo, esta tecnología se utiliza a menudo para enviar spam. Los emisores pueden aprovecharla para, por ejemplo, acumular direcciones de correo electrónico y enviar mensajes de spam a estos destinatarios.\n\nPara bloquear el scraping, los operadores de sitios web pueden adoptar diferentes medidas. Por ejemplo, el archivo robots.txt se utiliza para bloquear bots de buscadores. Por lo tanto, el scraping automático también se evita por medio de bots de software. Asimismo, es posible bloquear direcciones IP de bots. Los datos de contacto e información personal se pueden ocultar de manera selectiva: los datos sensibles, tales como los números de teléfono, se pueden proporcionar en forma de imagen o como CSS, lo que dificulta el scraping de los datos. Además, existen numerosos proveedores de servicios antibotde pago que pueden establecer un firewall. Con Google Search Console también es posible configurar notificaciones para informar a los operadores de sitios web en caso de que sus datos se utilicen para scraping.\n\n\n        Con una cuota de mercado de más del 80 por ciento, PHP se convierte para muchos programadores en el lenguaje de programación por excelencia a la hora de crear contenidos web dinámicos. Independientemente de que se quiera crear una página web propia, gestionar un foro de Internet o una tienda online, a menudo el software utilizado para dichos fines suele basarse en PHP. Nuestro tutorial sobre PHP...\n    \n\n\n        Para mejorar el rendimiento al realizar consultas reiteradas en SQL y, al mismo tiempo, minimizar el riesgo de manipulación al acceder a la base de datos, se recurre a las llamadas sentencias preparadas o Estas plantillas existen para muchos lenguajes, entre ellos PHP. ¿Qué son exactamente las sentencias preparadas y cómo pueden usarse en la gestión de bases de datos? \n    \n\n\n        El método conocido como web scraping permite extraer información de páginas web de forma automática. Para llevarlo a cabo, el lenguaje Python resulta especialmente útil. Sigue leyendo para saber por qué Python es tan buena elección, qué riesgos legales conlleva este método y qué alternativas existen. También puedes utilizar nuestro tutorial de web scraping con Python para dar tus primeros pasos...\n    \n\n\n"
     ]
    }
   ],
   "source": [
    "Content = ''\n",
    "for tag in soup.find_all('p'):\n",
    "    if(len(tag.text) > 144):\n",
    "        Content += tag.text+'\\n\\n'\n",
    "print(Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = Content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw) # convertir corpus a una lista de sentencias (tekenizar en sentencias)\n",
    "word_tokens = nltk.word_tokenize(raw) # convertir corpus a una lista de palabras (tokenizar en palabras)\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation) #remover signos de puntuacion (utilizamos la )\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens] # Lematizar palabras tokenizadas\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinar la similutud del texto inserdado y el corpus\n",
    "\n",
    "def response(user_response):# funcion respuesta (entrada: mensje de usuario)\n",
    "    chatbot_response = '' #definir la respuesta del chatbot\n",
    "    sent_tokens.append(user_response)# al listado de sentencias del corpus añadir al final de la lista el mensaje del usuario\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize,stop_words=stopwords.words('spanish'))# Lematizar y eliminar palbras\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "\n",
    "    # print(\"a\",tfidf[-1])\n",
    "    # print(\"------------------\")\n",
    "    # print(tfidf)\n",
    "    # 3 Evaluar similitud coseno entre mensaje usuario (tfid[-1]) y el corpus(tfidf)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "   \n",
    "\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    \n",
    "    flat.sort()\n",
    "  \n",
    "    req_tfidf = flat[-2]\n",
    "    # print(\"------------------\")\n",
    "    # print(req_tfidf)\n",
    "    # print(idx)\n",
    "    if(req_tfidf == 0):\n",
    "        chatbot_response = chatbot_response + \"Lo siento, no te he entendido\"\n",
    "        return chatbot_response\n",
    "    \n",
    "    else:\n",
    "        chatbot_response = chatbot_response + sent_tokens[idx]\n",
    "        return chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALUDOS_INPUTS = (\"hola\",\"buenas\",\"saludos\",\"que tal\",\"hey\",\"buenos dias\")\n",
    "SALUDOS_OUTPUTS = [\"hola\",\"hola, que tal?\",\"Hola, Como te puedo ayudar?\",\"hola, encantado de hablar contigo\"]\n",
    "\n",
    "def saludos(sentence):\n",
    "    for word in sentence.split():\n",
    "        # print(word.lower() in SALUDOS_INPUTS)\n",
    "        if word.lower() in SALUDOS_INPUTS:\n",
    "            return random.choice(SALUDOS_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AGENTE: MI NOMBRE ES AGENTE. CONTESTARE A SUS PREGUNTAS ACERCA DEL COVID Y LAS MEDIDAS DE SIGUIRDAD.\n",
      "AGENTE: *para garantizar la legalidad del web scraping, hay que tener en consideración lo siguiente:\n",
      "\n",
      "aunque el scraping está permitido en muchos casos, puede utilizarse con fines destructivos o incluso ilegales.*\n",
      "AGENTE: *el scrapingno siempre es legal.*\n",
      "AGENTE: *Lo siento, no te he entendido*\n",
      "AGENTE: Nos vemos pronto, !Gracias por su visita¡\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "\n",
    "print(\"AGENTE: MI NOMBRE ES AGENTE. CONTESTARE A SUS PREGUNTAS ACERCA DEL COVID Y LAS MEDIDAS DE SIGUIRDAD.\")\n",
    "while(flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower() #convertimos a minuscula\n",
    "\n",
    "    if(user_response not in [\"salir\",\"chao\",\"adios\",\"bay\",\"exit\"]):\n",
    "        if(user_response == 'gracias' or user_response == 'muchas gracias'): #se podria haber definido otra funcion de coincidencia\n",
    "            flag = True\n",
    "            print(\"AGENTE: No hay de que\")\n",
    "        else:\n",
    "            if(saludos(user_response) != None): # si la palabra insertada por el usuario es un saludo (coincidencia manuales)\n",
    "                print(\"AGENTE: \"+saludos(user_response))\n",
    "            else:#si la palabra insertada no es un saludo --> Corpus\n",
    "                print(\"AGENTE: \",end =\"*\")\n",
    "                print(response(user_response),end=\"*\\n\")\n",
    "                sent_tokens.remove(user_response) # para eliminar del corpus la respues del usuario y volver a evaluar \n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"AGENTE: Nos vemos pronto, !Gracias por su visita¡\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}